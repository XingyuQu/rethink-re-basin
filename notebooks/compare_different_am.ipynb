{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example comparing different implementations of the activation matching, including: \n",
    "\n",
    "1. The one in the \"Git Re-Basin: Merging Models modulo Permutation Symmetries\" paper,\n",
    "\n",
    "2. The one in\"REPAIR: REnormalizing Permuted Activations for Interpolation Repair\" paper,\n",
    "\n",
    "3. Ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from source.utils.utils import load_model\n",
    "from source.utils.data_funcs import load_data\n",
    "from source.utils.activation_matching import activation_matching\n",
    "from source.utils.connect import interpolate_state_dicts\n",
    "from source.utils.logger import Logger\n",
    "from torch.utils.data import DataLoader\n",
    "from source.utils.train import validate\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    model = 'cifar_vgg16'\n",
    "    dataset = 'cifar10'\n",
    "    special_init = None # 'vgg_init' (kaiming init) or None (uniform init)\n",
    "    print_freq = 100\n",
    "    data_dir = '../../Linear_Mode_Connectivity/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# necessary to create logger if using the train/validate/eval_line etc. functions\n",
    "Logger.setup_logging()\n",
    "logger = Logger()\n",
    "\n",
    "trainset, testset = load_data(config.data_dir, config.dataset)\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=256, shuffle=False)\n",
    "\n",
    "trainset_noaug, _ = load_data(config.data_dir, config.dataset, no_random_aug=True)\n",
    "trainloader_noaug = DataLoader(trainset_noaug, batch_size=128, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_barrier(end_1, end_2, mid, type='loss'):\n",
    "    if type == 'loss':\n",
    "        return mid - (end_1 + end_2) / 2\n",
    "    elif type == 'acc':\n",
    "        return (end_1 + end_2) / 2 - mid\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation code for matching VGG type of networks from the paper \"REPAIR: REnormalizing Permuted Activations for Interpolation Repair\"\n",
    "\n",
    "Repo: https://github.com/KellerJordan/REPAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# given two networks net0, net1 which each output a feature map of shape NxCxWxH\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the outputs of the two networks\n",
    "def run_corr_matrix(net0, net1, epochs=1, norm=True, loader=trainloader_noaug):\n",
    "    n = epochs*len(loader)\n",
    "    mean0 = mean1 = std0 = std1 = None\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for _ in range(epochs):\n",
    "            for i, (images, _) in enumerate(tqdm(loader)):\n",
    "                img_t = images.float().cuda()\n",
    "                out0 = net0(img_t)\n",
    "                out0 = out0.reshape(out0.shape[0], out0.shape[1], -1).permute(0, 2, 1)\n",
    "                out0 = out0.reshape(-1, out0.shape[2]).double()\n",
    "\n",
    "                out1 = net1(img_t)\n",
    "                out1 = out1.reshape(out1.shape[0], out1.shape[1], -1).permute(0, 2, 1)\n",
    "                out1 = out1.reshape(-1, out1.shape[2]).double()\n",
    "\n",
    "                mean0_b = out0.mean(dim=0)\n",
    "                mean1_b = out1.mean(dim=0)\n",
    "                std0_b = out0.std(dim=0)\n",
    "                std1_b = out1.std(dim=0)\n",
    "                outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "\n",
    "                if i == 0:\n",
    "                    mean0 = torch.zeros_like(mean0_b)\n",
    "                    mean1 = torch.zeros_like(mean1_b)\n",
    "                    std0 = torch.zeros_like(std0_b)\n",
    "                    std1 = torch.zeros_like(std1_b)\n",
    "                    outer = torch.zeros_like(outer_b)\n",
    "                mean0 += mean0_b / n\n",
    "                mean1 += mean1_b / n\n",
    "                std0 += std0_b / n\n",
    "                std1 += std1_b / n\n",
    "                outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    if norm:\n",
    "        corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "        return corr\n",
    "    else:\n",
    "        return cov\n",
    "\n",
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    return perm_map\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)\n",
    "\n",
    "# modifies the weight matrices of a convolution and batchnorm\n",
    "# layer given a permutation of the output channels\n",
    "def permute_output(perm_map, conv, bn):\n",
    "    pre_weights = [\n",
    "        conv.weight,\n",
    "    ]\n",
    "    if conv.bias is not None:\n",
    "        pre_weights.append(conv.bias)\n",
    "    if bn is not None:\n",
    "        pre_weights.extend([\n",
    "            bn.weight,\n",
    "            bn.bias,\n",
    "            bn.running_mean,\n",
    "            bn.running_var,\n",
    "        ])\n",
    "    for w in pre_weights:\n",
    "        w.data = w[perm_map]\n",
    "\n",
    "# modifies the weight matrix of a layer for a given permutation of the input channels\n",
    "# works for both conv2d and linear\n",
    "def permute_input(perm_map, layer):\n",
    "    w = layer.weight\n",
    "    w.data = w[:, perm_map]\n",
    "\n",
    "def subnet(model, n_layers):\n",
    "    return model.features[:n_layers]\n",
    "\n",
    "def activation_matching_v1(model_1, model_2):\n",
    "    model0 = model_1\n",
    "    model1 = deepcopy(model_2)\n",
    "\n",
    "    feats1 = model1.features\n",
    "\n",
    "    n = len(feats1)\n",
    "    for i in range(n):\n",
    "        layer = feats1[i]\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            # get permutation and permute output of conv and maybe bn\n",
    "            if isinstance(feats1[i+1], nn.BatchNorm2d):\n",
    "                assert isinstance(feats1[i+2], nn.ReLU)\n",
    "                perm_map = get_layer_perm(subnet(model0, i+3), subnet(model1, i+3))\n",
    "                permute_output(perm_map, feats1[i], feats1[i+1])\n",
    "            else:\n",
    "                assert isinstance(feats1[i+1], nn.ReLU)\n",
    "                perm_map = get_layer_perm(subnet(model0, i+2), subnet(model1, i+2))\n",
    "                permute_output(perm_map, feats1[i], None)\n",
    "            # look for succeeding layer to permute input\n",
    "            next_layer = None\n",
    "            for j in range(i+1, n):\n",
    "                if isinstance(feats1[j], nn.Conv2d):\n",
    "                    next_layer = feats1[j]\n",
    "                    break\n",
    "            if next_layer is None:\n",
    "                next_layer = model1.classifier[0]\n",
    "            permute_input(perm_map, next_layer)\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_1 = torch.load(f'../../Linear_Mode_Connectivity/same_init_ex/{config.dataset}/{config.model}/diff_init/seed_20/model_1_160.pt', map_location=device)\n",
    "sd_2 = torch.load(f'../../Linear_Mode_Connectivity/same_init_ex/{config.dataset}/{config.model}/diff_init/seed_20/model_2_160.pt', map_location=device)\n",
    "\n",
    "model_1 = load_model(config).to(device)\n",
    "model_2 = load_model(config).to(device)\n",
    "model_1.load_state_dict(sd_1)\n",
    "model_2.load_state_dict(sd_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "sd_2_am, _ = activation_matching(config.model, model_1, model_2, trainloader_noaug, print_freq=100, device=device)\n",
    "am_time = time() - start\n",
    "\n",
    "start = time()\n",
    "sd_2_git_am, _ = activation_matching(config.model, model_1, model_2, trainloader_noaug, print_freq=100, device=device, type='git')\n",
    "git_am_time = time() - start\n",
    "\n",
    "start = time()\n",
    "model_2_repair_am = activation_matching_v1(model_1, model_2) \n",
    "sd_2_repair_am = model_2_repair_am.state_dict()\n",
    "repair_am_time = time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_am_mid = interpolate_state_dicts(sd_1, sd_2_am, 0.5)\n",
    "sd_git_am_mid = interpolate_state_dicts(sd_1, sd_2_git_am, 0.5)\n",
    "sd_repair_am_mid = interpolate_state_dicts(sd_1, sd_2_repair_am, 0.5)\n",
    "\n",
    "model_am_mid = load_model(config).to(device)\n",
    "model_git_am_mid = load_model(config).to(device)\n",
    "model_repair_am_mid = load_model(config).to(device)\n",
    "\n",
    "model_am_mid.load_state_dict(sd_am_mid)\n",
    "model_git_am_mid.load_state_dict(sd_git_am_mid)\n",
    "model_repair_am_mid.load_state_dict(sd_repair_am_mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_1, train_acc_1, _, _ = validate(trainloader_noaug, model_1, criterion, device, config)\n",
    "test_loss_1, test_acc_1, _, _ = validate(testloader, model_1, criterion, device, config)\n",
    "train_loss_2, train_acc_2, _, _ = validate(trainloader_noaug, model_2, criterion, device, config)\n",
    "test_loss_2, test_acc_2, _, _ = validate(testloader, model_2, criterion, device, config)\n",
    "\n",
    "train_loss_am_mid, train_acc_am_mid, _, _ = validate(trainloader_noaug, model_am_mid, criterion, device, config)\n",
    "test_loss_am_mid, test_acc_am_mid, _, _ = validate(testloader, model_am_mid, criterion, device, config)\n",
    "train_loss_git_am_mid, train_acc_git_am_mid, _, _ = validate(trainloader_noaug, model_git_am_mid, criterion, device, config)\n",
    "test_loss_git_am_mid, test_acc_git_am_mid, _, _ = validate(testloader, model_git_am_mid, criterion, device, config)\n",
    "train_loss_repair_am_mid, train_acc_repair_am_mid, _, _ = validate(trainloader_noaug, model_repair_am_mid, criterion, device, config)\n",
    "test_loss_repair_am_mid, test_acc_repair_am_mid, _, _ = validate(testloader, model_repair_am_mid, criterion, device, config)\n",
    "\n",
    "end_1 = torch.tensor([train_loss_1, train_acc_1, test_loss_1, test_acc_1])\n",
    "end_2 = torch.tensor([train_loss_2, train_acc_2, test_loss_2, test_acc_2])\n",
    "end_am_mid = torch.tensor([train_loss_am_mid, train_acc_am_mid, test_loss_am_mid, test_acc_am_mid])\n",
    "end_git_am_mid = torch.tensor([train_loss_git_am_mid, train_acc_git_am_mid, test_loss_git_am_mid, test_acc_git_am_mid])\n",
    "end_repair_am_mid = torch.tensor([train_loss_repair_am_mid, train_acc_repair_am_mid, test_loss_repair_am_mid, test_acc_repair_am_mid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "barrier_am = [None] * 4\n",
    "barrier_git_am = [None] * 4\n",
    "barrier_repair_am = [None] * 4\n",
    "\n",
    "barrier_am[0] = cal_barrier(train_loss_1, train_loss_2, train_loss_am_mid, type='loss')\n",
    "barrier_am[1] = cal_barrier(train_acc_1, train_acc_2, train_acc_am_mid, type='acc')\n",
    "barrier_am[2] = cal_barrier(test_loss_1, test_loss_2, test_loss_am_mid, type='loss')\n",
    "barrier_am[3] = cal_barrier(test_acc_1, test_acc_2, test_acc_am_mid, type='acc')\n",
    "\n",
    "barrier_git_am[0] = cal_barrier(train_loss_1, train_loss_2, train_loss_git_am_mid, type='loss')\n",
    "barrier_git_am[1] = cal_barrier(train_acc_1, train_acc_2, train_acc_git_am_mid, type='acc')\n",
    "barrier_git_am[2] = cal_barrier(test_loss_1, test_loss_2, test_loss_git_am_mid, type='loss')\n",
    "barrier_git_am[3] = cal_barrier(test_acc_1, test_acc_2, test_acc_git_am_mid, type='acc')\n",
    "\n",
    "barrier_repair_am[0] = cal_barrier(train_loss_1, train_loss_2, train_loss_repair_am_mid, type='loss')\n",
    "barrier_repair_am[1] = cal_barrier(train_acc_1, train_acc_2, train_acc_repair_am_mid, type='acc')\n",
    "barrier_repair_am[2] = cal_barrier(test_loss_1, test_loss_2, test_loss_repair_am_mid, type='loss')\n",
    "barrier_repair_am[3] = cal_barrier(test_acc_1, test_acc_2, test_acc_repair_am_mid, type='acc')\n",
    "\n",
    "barrier_am = torch.tensor(barrier_am)\n",
    "barrier_git_am = torch.tensor(barrier_git_am)\n",
    "barrier_repair_am = torch.tensor(barrier_repair_am)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (ours): 28.48s\n",
      "Time (from Git): 14.34s\n",
      "Time (from REPAIR): 81.37s\n",
      "Test Accuracy:\n",
      "Barrier (ours): 42.22%\n",
      "Barrier (from Git): 67.47%\n",
      "Barrier (from REPAIR): 42.28%\n"
     ]
    }
   ],
   "source": [
    "print(f'Time (ours): {am_time:.2f}s')\n",
    "print(f'Time (from Git): {git_am_time:.2f}s')\n",
    "print(f'Time (from REPAIR): {repair_am_time:.2f}s')\n",
    "print('Test Accuracy:')\n",
    "print(f'Barrier (ours): {barrier_am[-1]:.2f}%')\n",
    "print(f'Barrier (from Git): {barrier_git_am[-1]:.2f}%')  \n",
    "print(f'Barrier (from REPAIR): {barrier_repair_am[-1]:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
